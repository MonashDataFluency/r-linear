---
title: "Linear models in R (slideshow)"
author: "Paul Harrison @paulfharrison"
output: 
    ioslides_presentation:
        widescreen: true
        smaller: true
        css: style.css
        incremental: true
---

## Linear models

```{r, include=F}
knitr::opts_chunk$set(fig.width=5, fig.height=5)
```

"linear model" "general linear model" "linear predictor" "regression" "multiple regression" "multiple linear regression" "multiple linear regression model"

Learn to predict a response variable as

* a straight line relationship with a predictor variable
* or more than one predictor variable
* actually it doesn't have to be a straight line
* some of the predictors could be categorical (ANOVA)
* and there could be interactions

<br/>

* test which variables and interactions the data supports as predictors
* give a confidence interval on any estimates


## Linear models in R

<div style="float: right; width: 25%; padding 2em;">
```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

Many features of the S language (predecessor to R) were created to support working with linear models and their generalizations:

<br>

>* `data.frame` type introduced to hold data for modelling.

>* `factor` type introduced to hold categorical data.

>* `y ~ x1+x2+x3` formula syntax specify terms in models.

>* Manipulation of "S3" objects holding fitted models.

>* Rich set of diagnostic visualization functions.

<br>

Primary reference is "Statistical models in S".


## Linear maths and R

<!--
Will give everything today in English, in maths, and in R code.
-->

We will be using vectors and matrices extensively today.

In mathematics, we usually treat a vector as a matrix with a single column. In R, they are two different types. <span class="tip">*<span class="tiptext">R also makes a distinction between `matrix` and `data.frame` types. There is a good chance you have used data frames but not matrices before now.<br><br>Matrices contain all the same type of value, typically numeric, whereas data frames can have different types in different columns.</span></span>

A matrix can be created from a vector using `matrix`, or from multiple vectors with `cbind` (bind columns) or `rbind` (bind rows), or from a data frame with `as.matrix`.

Matrix transpose exchanges rows and columns. In maths it is indicated with a small t, eg $X^\top$. In R use the `t` function, eg `t(X)`.

$$
X = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix} 
\quad \quad X^\top = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} 
$$

## Linear maths and R

Taking the **dot product** of two vectors we multiply corresponding elements together and add the results to obtain a total.

In mathematical notation:

$$
a^\top b = \sum_{i=1}^{n} a_i b_i
$$

In R:

```{r eval=F}
sum(a*b)
```

## Geometry

<div style="float: left; width: 25%; padding: 2em">
The dot product is our ruler and set-square.

```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/Triangle-Ruler.png")
```
</div>

### Lengths

A vector can be thought of as an arrow in a space.

The dot product of a vector with itself $a^\top a$ is the square of its length. Pythagorus!

### Right angles

Two vectors at right angles have a dot product of zero. They are **orthogonal**.


## Linear maths and R

Taking the product of a matrix $X$ and a vector $a$ with length matching the number of columns, the result is a vector containing the dot product of each row of the matrix $X$ with the vector $a$.

Can also think of this as a weighted sum of the columns of $X$.

$$
Xa = 
a_1
\begin{bmatrix}
x_{1,1} \\
x_{2,1} \\
\vdots
\end{bmatrix}
+ a_2
\begin{bmatrix}
x_{1,2} \\
x_{2,2} \\
\vdots
\end{bmatrix}
+
\dots
$$

In R:

```{r eval=F}
X %*% a
```

<br>
(It's also possible to multiply two matrices with `%*%` but we won't need this today.)



## Geometry - subspaces
<div style="float: right">
*Example: a 3x2 matrix puts<br>2D vectors into a 2D subspace in 3D.*

```{r echo=F, fig.width=2, fig.height=4}
library(plot3D)
x <- seq(-3,3,by=1)
y <- seq(-3,3,by=1)
m <- mesh(x,y)
z <- m$x + m$y
op <- par(mfrow=c(2,1), mar=c(1,1,1,1))
plot(m$x, m$y, xlab="", ylab="", xaxt="n", yaxt="n")
surf3D(m$x,m$y,z,border="black",col="#00aacc",colvar=NA,bty="f")
par(op)
```
</div>

> * A line in two or more dimensions, passing through the origin.
> * A plane in three or more dimensions, passing through the origin.
> * A point at the origin.

These are all examples of **subspaces**.

Think of all the vectors that could result from multiplying a matrix $X$ with some arbitrary vector. 

If the matrix $X$ has $n$ rows and $p$ columns, we obtain an (at most) $p$-dimensional **subspace** within an $n$-dimensional space.

A subspace has an **orthogonal subspace** with $n-p$ dimensions. All the vectors in a subspace are orthogonal to all the vectors in its orthogonal subspace.


##

Do section: Vectors and matrices


## Models

A **model** can be used to predict a **response** variable based on a set of **predictor** variables. <span class="tip">*<span class="tiptext">Alternative terms: depedent variable, independent variables.<br><br>We are using causal language, but really only describing an association.</span></span>


The prediction will usually be imperfect, due to random noise.

<br>



## Linear model

A **response** $y$ is produced based on $p$ **predictors** $x_j$ plus noise $\varepsilon$ ("epsilon"):

$$ y = \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \varepsilon $$

The model has $p$ **terms**, plus a noise term. The model is specified by the choice of **coefficients** $\beta$ ("beta").

This can also be written as a dot product:

$$ y = \beta^\top x + \varepsilon $$

<br>

The noise is assumed to be normally distributed with standard deviation $\sigma$ ("sigma") (i.e. variance $\sigma^2$):

$$ \varepsilon \sim \mathcal{N}(0,\sigma^2) $$

<br>

Typically $x_1$ will always be 1, so $\beta_1$ is a constant term in the model. We still count it as one of the $p$ predictors. <span class="tip">*<span class="tiptext">This matches what R does, but may differ from other presentations!</span></span>


## Linear model in R code

For vector of coefficients `beta` and vector of predictors in some particular case `x`, the most probable outcome is:

```{r eval=F}
y_predicted <- sum(beta*x)
```

A simulated possible outcome can be generated by adding random noise:

```{r eval=F}
y_simulated <- sum(beta*x) + rnorm(1, mean=0, sd=sigma)
```

But where do the coefficients $\beta$ come from?


## Model fitting -- estimating $\beta$

Say we have observed $n$ responses $y_i$ with corresponding vectors of predictors $x_i$: 

$$ 
\begin{align}
y_1 &= \beta_1 x_{1,1} + \beta_2 x_{1,2} + \dots + \beta_p x_{1,p} + \varepsilon_1 \\
y_2 &= \beta_1 x_{2,1} + \beta_2 x_{2,2} + \dots + \beta_p x_{2,p} + \varepsilon_2 \\
    & \dots \\
y_n &= \beta_1 x_{n,1} + \beta_2 x_{n,2} + \dots + \beta_p x_{n,p} + \varepsilon_n 
\end{align}
$$

This is conveniently written in terms of a vector of responses $y$ and matrix of predictors $X$:

$$ y = X \beta + \varepsilon $$

Each response is assumed to contain the same amount of noise:

$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

## Model fitting -- estimating $\beta$ with geometry

$$ y = X \beta + \varepsilon $$
$$ \varepsilon_i \sim \mathcal{N}(0,\sigma^2) $$

**Maximum Likelihood**<span class="tip">*<span class="tiptext">"Likelihood" has a technical meaning in statistics: it is the probability of the *data* given the model parameters. This is backwards from normal English usage.</span></span> estimate:<br>
We choose $\hat\beta$ to maximize the likelihood of $y$.

A very nice consequence of assuming normally distributed noise is that the vector $\varepsilon$ has a spherical distribution, so the choice of $\hat\beta$ making $y$ most likely is the one that places $X \hat\beta$ nearest to $y$

Distance is the square root of the sum of squared differences in each dimension, so this is also called a **least squares** estimate. We choose $\hat \beta$ to minimize $\hat \varepsilon^\top \hat \varepsilon$.

$$ \hat \varepsilon = y - X \hat \beta $$


<br>
<br>
<div style="font-size: 60%;">
(This is *one route* to least squares. Normally distributed noise is [not a strict requirement](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem), so long as there aren't wild outliers.)</div>

## Model fitting in R with `lm( )`

Our starting point for modelling is usually a data frame, but the process of getting $y$ and $X$ from the data frame has some complications, and a special "formula" syntax. 

```
        data frame

            ||
            || Weirdness involving "~" formula syntax happens
            \/

           y, X

            ||
            || Actual magic of model fitting happens
            \/

        coef, sigma
```

First look at examples then discuss process in detail.

##

Do section: Single numerical predictor


## Formula syntax introduction

We've just seen the formula

`height ~ age`

> * Left hand side specifies response $y$.
> * Right hand side specifies model matrix $X$.

Terms refer to columns of a data frame or variables in the environment.

There are some rules to learn about the construction of $X$.


## Formula syntax -- intercept term

From the formula `height ~ age`, X had *two* columns. What gives? 

Intercept term `1` is implicit (predictor consisting of all 1s). 

> * `height ~ 1 + age` to be explicit.

Omit with `-1` or `0`. 

> * `height ~ 0 + age` to force a model where newborns are 0cm tall.


## Formulas - factors

Factors in R represent categorical data, such as treatment groups or batches in an experiment.

R converts a factor into $n_\text{levels}-1$ columns of "indicator variables", aka "one-hot" encoding. <span class="tip">\*<span class="tiptext">The encoding can be adjusted using the `C()` function within a formula. The default differs between S and R languages!</span></span>

```{r}
f <- factor(c("a","a","b","b","c","c"))

model.matrix(~ f)
```

## Formulas - factors

Need to be careful interpreting the meaning of coefficients for factors.

> * `(Intercept)` is the mean for "a".
> * `fb` is the *difference* between "b" and "a".
> * `fc` is the *difference* between "c" and "a".

```{r}
f <- factor(c("a","a","b","b","c","c"))

model.matrix(~ f)
```

## Formulas - factors without the intercept term

R tries to be clever about factor encoding if the intercept is omitted.

Without an intercept, each coefficient is simply the mean for that level. 

Can't do this with multiple factors.<span class="tip">*<span class="tiptext">Without the intercept term, R will only do this for the first factor. Further factors are encoded as before, omitting the first level. If R didn't do this, there would be multiple choices of coefficients able to make the exact same prediction (the predictors would not be linearly independent).</span></span>

```{r}
f <- factor(c("a","a","b","b","c","c"))
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ f)
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ 0 + f)
```
</div>


## 

Do section: Single factor predictor, two levels


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
Imagine an experiment in which two noisy measurements of something are made.

Imagine many runs of this experiment.

The runs form a fuzzy circular cloud around the (noise-free) truth.
</div>

```{r echo=F}
source("../diagram.R")
begin()
many_exp()
the_truth()
```

## Model fitting -- estimating $\beta$ with geometry

```{r echo=F}
begin()
one_exp()
```


## Model fitting -- estimating $\beta$ with geometry

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit <- lm(y ~ 1)

coef(fit)
predict(fit)
residuals(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
```

## Model fitting -- estimating $\sigma$

```{asis}
<div style="float: right; width: 15em;">
The coefficient is wrong, but is our best estimate. It is an unbiassed estimate.

The residuals vector must be orthogonal to the subspace of possible predictions so it is too short, but we can correct for this to get an unbiassed estimate of the variance.
<span class="tip">*<span class="tiptext">In R, this is done using QR decomposition. X is decomposed into $X=QR$ with $Q$ orthonormal (i.e. a rotation matrix). $Q^{\top}y$ rotates $y$ so that the first $p$ elements contain all the information relevant to fitting the model, and the remaining $n-p$ elements are purely noise.<br><br>The first $p$ elements of $Q^{\top}\hat{\varepsilon}$ will be zero due to fitting the model, but the remaining $n-p$ elements are unaffected by this and as this is a rotation of a spherically symmetric noise distribution they have the same variance as the original elements of $\varepsilon$.</span></span>


$$ \hat \sigma^2 = { \hat\varepsilon^\top \hat\varepsilon \over n-p } $$
```

```{r}
df.residual(fit)  # n-p
sigma(fit)
```
</div>

```{r echo=F}
begin()
to_predict()
the_truth()
```



## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)
```

Two model formulas representing null hypothesis H0 and alternative hypothesis H1.

H0 must *nest* within H1.

$\hat\varepsilon_0$ can't be shorter than $\hat\varepsilon_1$

Is it *surprisingly longer*? Then we reject H0.
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses

<div style="float: right; width: 15em;">
```{r}
y <- c(3,5)
fit0 <- lm(y ~ 0)
fit1 <- lm(y ~ 1)

anova(fit0, fit1)
```

Here we can not reject the null hypothesis (F(1,1)=16, p=0.156). <span class="tip">*<span class="tiptext">If H0 is true, we expect F to be close to 1. This value of F is much larger than 1. Why can't we reject H0? The problem is that we are very uncertain about $\sigma$ because our residual degrees of freedom is so low. Hence the F(1,1) distribution here has a very long fat tail.</span></span>
</div>

```{r echo=F}
begin()
to_test()
```

## Testing hypotheses -- the gory details

<div style="float: right; width: 15em">
```{r}
df0 <- df.residual(fit0)       # n-p0
RSS0 <- sum(residuals(fit0)^2)

df1 <- df.residual(fit1)       # n-p1
RSS1 <- sum(residuals(fit1)^2)

F <- (RSS0-RSS1)/(df0-df1) / (RSS1/df1) 
F
pf(F, df0-df1, df1, lower.tail=FALSE)
```
</div>
$$
RSS_0 = \hat\varepsilon_0^\top \hat\varepsilon_0 \\
RSS_1 = \hat\varepsilon_1^\top \hat\varepsilon_1
$$

<br>

$$
F = { {RSS_0-RSS_1 \over p_1-p_0} \over {RSS_1 \over n-p_1} }
$$

RSS = Residual Sum of Squares

Is the increase in RSS in H0 merely proportionate 
<br>to the extra degree(s) of freedom?

<div style="font-size: 150%">`anova(fit0, fit1)`<br>handles all this for us.</div>



## A different way to test: linear hypotheses

"linear hypothesis" "general linear hypothesis" "linear restriction" "contrast"

<br>
A **linear hypothesis** test tests an H0 in which some linear combination of coefficients in H1 is restricted to be zero.

<br>
From a linear combination of coefficients we obtain:

>* estimated value (calculated with dot product)
>* Confidence Interval (CI)
>* hypothesis test p-value

<br>
Examples:

>* a **contrast** compares specific levels of an experimental factor<span class="tip">*<span class="tiptext">Terminology is a little tricky here. The limma package refers to linear combinations in general as contrasts.</span></span>
>* the difference of response between two imagined individuals

<br>
Use `multcomp` package.


##

Do section: Multiple factors, many levels


## What is a p-value anyway?

A p-value lets us choose our probability of falsely rejecting a true hypothesis.

If H1 is true, $p$ tends towards zero.

If H0 is true, $p$ is a uniformly random number between 0 and 1.

<br>

If we decide to reject H0 when $p \leq 0.05$, we are guaranteed a probability of 0.05 of rejecting H0 if it is actually true.

<br>

Suppose we perform 20 such tests. There is a high chance of falsely rejecting at least one null hypothesis.

Need to adjust p-values for this problem (see `p.adjust( )`).



## Multiple testing

Suppose some set of hypotheses are true. What chance do we run of rejecting one or more of them?

### Hypotheses are mutually exclusive

> * Only one can be true, so no problem!

Example: reject all values outside a confidence interval.

### Family-Wise Error Rate (FWER) control

"*Any* false rejection will ruin my argument."

> * Tukey's Honestly Significant Differences
> * Bonferroni correction (etc)

### False Discovery Rate (FDR) control

"I can tolerate a certain *rate* of false rejections amongst rejected hypotheses."

> * Benjamini and Hochberg correction (etc)


## More on formulas

"Wilikins-Rogers notation"

Merely a convenience, if you already have model matrix `X` can just use `y ~ 0 + X`.

We've seen:

> * intercept term
> * numerical predictors
> * factor predictors
> * interactions between factors

Let's look at other things formula syntax can do.

## Formulas - data types

`numeric` vector is represented as itself.

`logical` vector is converted to 0s and 1s.

`factor` is represented as $n_\text{levels}-1$ columns containing 1s and 0s. 

`matrix` is represented as multiple columns.


## Formulas - calculations and function calls

Can calculate with function calls, or enclose maths in `I( )`.

Handy functions such as `poly` and `splines::ns` produce matrices for fitting curves.

```{r}
x <- 1:6
```

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ x + I(x^2) + log2(x))
```
</div>

<div style="float: left; width: 15em;">
```{r}
model.matrix(~ poly(x,3))
```
</div>


## Formulas - interactions

`a:b` specifies an interaction between `a` and `b`. 

All pairings of predictors from the two terms are multiplied together.

For logical and factor vectors, this produces the logical "and" of each pairing.
<br>&nbsp;

<br>

```{r}
model.matrix(~ f + x + f:x)
```

## Formulas - interactions

`a*b` is shorthand to also include main effects `a + b + a:b`.

(More obscurely `a/b` is shorthand for `a + a:b`, if `b` only makes sense in the context of each specific `a`.)

<span style="display: inline-block; width: 6em">`(a+b+c)^2`</span> is shorthand for `a + b + c + a:b + a:c + b:c`.
<br>
<span style="display: inline-block; width: 6em">`a*b*c`</span> is shorthand for `a + b + c + a:b + a:c + b:c + a:b:c`.

<br>

```{r}
model.matrix(~ f*x)
```


##

Do section: Gene expression example




## Extensions to the idea of linear models

Linear models are a building block or the basis for many more complex statistical models.

<br>

Of particular importance (in my opinion):

> * Generalized Linear Models
> * Linear Mixed Models
> * Screening many different response variables


## Generalized Linear Models (GLMs) 

**(Not covered today.)**

We are still trying to predict the mean value, but now the response has a distribution that changes based on its mean value.

We want to specify the model on a transformed scale, but the data contains values that can't be transformed.

Examples:

> * Fit a model on a log scale to count data that includes zeros using a *Poisson* or *negative-binomial GLM*.
> * Fit a model of the *log odds* of a binary outcome using *logistic regression*.

Fit using the function `glm`.

<br>

Survival analysis using Cox regression has some similarities.


## Linear Mixed Models

**(Not covered today.)**

The models we have fitted today contain only **fixed effects**.
<br>A **mixed model** contains both **fixed effects** and **random effects**.

>* A factor may represent individuals from a larger population.
>* There may be a source of random variation affecting multiple responses, such as a batch effect.

Using random effects:

> * Lets us characterize a population.<br>How does an average individual behave, and what is the standard deviation?<br>The model is not about *particular individuals*, but about *the population as a whole*.
> * Allows analysis of experiments with **nested** factors, such as repeated measurements (technical replicates).
> * Gives slightly improved estimates of random effects, by "shrinking" estimates toward the mean.
> * Gives slightly improved estimates of fixed effects, because modelling of random nuisance effects is better.

Fit using a package such as `lme4`.


## Screening many responses

"High throughput" biological datasets we can view as containing many sets of responses.

Typical example is RNA-Seq gene expression.

<img src="../figures/igv.png" height="450">


## Screening many responses

"High throughput" biological datasets we can view as containing many sets of responses.

Typical example is RNA-Seq gene expression.

> * Perform RNA-Seq on a set of biological samples.
> * Number of RNA molecules seen for each gene in each sample is counted.

> * Linear model formula based on experiment design.
> * Each gene can be viewed as a separate set of responses.

Key features:

> * Residual standard deviation is similar between genes. "Emprical Bayes" shrinkage of these standard deviations overcomes problems with small numbers of samples.
> * A test is performed for each gene. Multiple testing adjustment is critical!

<br>

Use Bioconductor packages such as `limma` (fit many linear models) or `edgeR` (fit many GLMs).


## 

Do section: Testing many genes with limma


## Summary

Should now be able to

>* Fit linear models.

>* Identify and correct problems with a model, or recognize the need for a more advanced method of modelling.

>* Test a hypothesis using two nested linear models.
>* Test a hypothesis and calculate a CI on a linear combination of coefficients.

>* Test a hypothesis for many sets of responses using limma.
>* Understand the need for multiple-testing correction.


## Further reading

<div style="float: right; width: 25%; padding 2em;">
```{r echo=F,out.width="100%",fig.align="left"}
knitr::include_graphics("../figures/chambers_and_hastie.jpg")
```
</div>

**Primary reference is:**

Chambers and Hastie (1991) "Statistical Models in S"

<br>
**Also good:**

Faraway (2014) "Linear Models with R"

James, Witten, Hastie and Tibshirani (2013) "An Introduction to Statistical Learning"



## Extra slides

## Extensions to linear models

### Insufficient data or $p > n$

>* Regularization with ridge regression and/or LASSO: `glmnet::glmnet( )`
>* Partial Least Squares (PLS) <br>(pick best direction to start moving in and take best leap along it, repeat)
>* Principal Component Regression (PCR)<br>(direction of largest variation in X first)

### Noise not normal (eg binary or count data)

>* **Generalized Linear Models:** `glm( )`<br>(logistic regression, binomial, Poisson, negative binomial, gamma, etc)
>* Quasi-Likelihood F-test in case of over-dispersion

### Survival time, with censoring

>* Cox survival model: `survival::coxph( )`


## Extensions to linear models

### Noise at a coarser grain than single measurements

Individuals tracked over time, repeated measurements, blocks/batches/plots, etc.

>* **Linear Mixed Model** (some coefficients are drawn from a random distribution): `lme4`, `lmerTest`
>* Time series models.

### Noise in data is poorly understood, or possibly misspecified

>* Quantile regression may be more robust to outliers.<br>(eg try to predict the median)
>* "Sandwich" estimator gives consistent estimates of standard error, even if variance is not uniform. 
>* "Bootstrapping" can estimate bias, provide confidence intervals for almost any method of estimation.


## Extensions to linear models

### Response is non-linear in predictors

>* Feature engineering: `poly( )`, `splines::ns( )`
>* Local regression (LOESS): `loess( )`
>* Generalized Additive Models (GAM): `mgcv::gam( )`<br>(splines with regularization)
>* Neural network (plug together many logistic regression models)

Also consider tree methods such as gradient boosted trees.






