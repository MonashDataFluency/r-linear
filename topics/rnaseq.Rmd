---
title: "RNA-Seq differential expression analysis"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r echo=FALSE}
knitr::opts_chunk$set(fig.align="center", fig.width=5,fig.height=5)
```

# Preparation

We load some new packages.

```{r message=F, warning=F}
library(edgeR)          # cpm, etc -- RNA-Seq normalization
library(limma)          # lmFit, etc -- fitting many models
library(ComplexHeatmap) # Heatmap
library(seriation)      # seriate -- nice clustering and ordering
library(reshape2)       # melt -- matrix to long data frame

library(multcomp)       # glht -- linear hypotheses
library(tidyverse)      # working with data frames, plotting
```


Setup as before with the teeth dataset, if you haven't already.

```{r}
teeth <- read_csv("r-linear-files/teeth.csv")

teeth$tooth <- factor(teeth$tooth, levels=c("lower","upper"))
teeth$mouse <- factor(teeth$mouse)

# A convenience to examine different model fits
more_data <- expand.grid(
    day=seq(14.3,18.2,by=0.01),
    tooth=factor(c("lower","upper"), levels=c("lower","upper")))

look <- function(y, fit=NULL) {
    p <- ggplot(teeth,aes(x=day,group=tooth))
    if (!is.null(fit)) {
        more_ci <- cbind(
            more_data, 
            predict(fit, more_data, interval="confidence"))
        p <- p + 
            geom_ribbon(aes(ymin=lwr,ymax=upr), data=more_ci, alpha=0.1) + 
            geom_line(aes(y=fit,color=tooth), data=more_ci)
    }
    p + geom_point(aes(y=y,color=tooth)) +
        labs(y=deparse(substitute(y)))
}
```


# Testing many genes with limma

In this section we look at fitting the same matrix of predictors X to many different sets of responses y. We will use the package `limma` from Bioconductor. This is a very brief demonstration, and there is much more to this package. See the RNAseq123 tutorial at https://bioconductor.org/packages/release/workflows/vignettes/RNAseq123/inst/doc/limmaWorkflow.html and the usersguide.pdf at https://bioconductor.org/packages/release/bioc/html/limma.html

## Load the data

Actually in the teeth dataset, the expression level of all genes was measured!

```{r warning=F, message=F}
counts_df <- read_csv("r-linear-files/teeth-read-counts.csv")
counts <- as.matrix( select(counts_df, -gene) )
rownames(counts) <- counts_df$gene

dim(counts)
counts[1:5,]
```

## Model matrix

The column names match our `teeth` data frame.

```{r}
colnames(counts)
teeth$sample
```

We will use limma to fit a linear model to each gene. The same model formula will be used in each case. limma doesn't automatically convert a formula into a model matrix, so we have to do this step manually. Here I am using a model formula that treats the upper and lower teeth as following a different linear trend over time.

I'm going to rescale the time from zero to one, to aid interpretation.

```{r}
teeth$time <- (teeth$day - 14.5) / (18-14.5)
teeth

X <- model.matrix(~ tooth * time, data=teeth)
X
```

`limma` refers to this matrix as the "design".


## Remove low expression genes

There is little chance of detecting differential expression in genes with very low read counts. Including these genes will require a larger False Discovery Rate correction, and also confuses limma's Empirical Bayes parameter estimation. `edgeR` provides a function `filterByExpr` that performs their recommended level of filtering. The filtering takes into account the complexity of the model being fitted, so the "design" matrix is also needed.

```{r}
keep <- filterByExpr(counts, design=X)
table(keep)

counts_kept <- counts[keep, ]
dim(counts_kept)
```


## Normalize, log transform

Different samples may have produced different numbers of reads. We now normalize this away by converting the read counts to Reads Per Million, and log2 transform the results. There are some subtleties here: "TMM" normalization is used as a small adjustment to the total number of reads in each sample. A small constant "prior count" is added to the counts to avoid calculating `log2(0)`. The `edgeR` and `limma` manuals describe these steps in more detail.

```{r warning=F, message=F}
dgelist <- calcNormFactors(DGEList(counts_kept))
dgelist$samples

log2_cpms <- cpm(dgelist, log=TRUE, prior.count=2)

log2_cpms[1:5,]
```


## Fitting a model to and testing each gene

We are now ready to fit a linear model to each gene.

```{r}
fit <- lmFit(log2_cpms, X)

class(fit)
fit$coefficients[1:5,]
```

Significance testing in limma is by the use of linear hypotheses (which limma refers to as "contrasts"). A difference between `glht` and limma's `contrasts.fit` is that limma expects the linear combinations to be in columns rather than rows.

We will first look for genes where the slope over time is not flat, *averaging* the lower and upper teeth.

```{r}
# Lower slope: c(0,0,1,0)
# Upper slope: c(0,0,1,1)

K <- rbind( average_slope=c(0,0,1,0.5) )
cfit <- contrasts.fit(fit, t(K))         #linear combinations in columns!
efit <- eBayes(cfit, trend=TRUE)
```

The call to `eBayes` does Empirical Bayes squeezing of the residual variance for each gene (see Appendix). This is a bit of magic that allows limma to work well with small numbers of samples. We're now ready to look at the results.

```{r}
topTable(efit)
```

The column `adj.P.Val` contains FDR adjusted p-values. 

The `logFC` column contains the estimate for the quantity we are interested in, usually this is a log2 fold change. Here, it's the log2 fold change from day 14.5 to day 18.

We can also ask for results for all genes:

```{r}
all_results <- topTable(efit, n=Inf)

significant <- all_results$adj.P.Val <= 0.05
table(significant)

# or

dt <- decideTests(efit)
summary(dt)
```


## Diagnostic plots

### MD plot

As always, diagnostic plots are an essential part of the process.

It's common to plot the `logFC` column against the average log expression level, an "MD plot" (also sometimes called an "MA plot").

```{r}
plotMD(efit, status=dt[,1])

# Or do it ourselves:
ggplot(all_results, aes(x=AveExpr, y=logFC)) + 
    geom_point(size=0.1) +
    geom_point(data=all_results[significant,], size=0.5, color="red")
```

### Volcano plot

Often people will choose genes based both on FDR and log fold change thresholds. The volcano plot provides a way to visualize this. Volcano plots show log fold change on the x-axis and p-values on the y-axis on a log scale. Usually the adjusted p-value (FDR) is used, but you may also sometimes see unadjusted p-values being used.

```{r}
chosen_results <- all_results |>
    filter(adj.P.Val <= 0.05, abs(logFC) >= 1)

ggplot(all_results, aes(x=logFC, y=-log10(adj.P.Val))) + 
    geom_point(size=0.1) +
    geom_point(data=chosen_results, size=0.5, color="red")
```

If p-values simply got smaller according to the magnitude of log fold change, the plot would be a perfect V shape. However because different genes have different residual standard deviation, the arms of the V are puffier.


### MDS plot

A commonly used first look at the quality of the data is provided by limma's MDS plot. This is very similar to a PCA plot. A difference is that it concentrates by default on the top 500 most different genes when comparing each pair of samples. The percentages shown on the axes refer to variance explained in these comparisons.

With the MDS plot we are looking for variables in the experiment like treatment groups and batch effects to be reflected in the layout. Here, pleasingly, dimension one corresponds to time. Some separation is seen between corresponding upper and lower molars. There seem to be other things going on in the second dimension that may represent unexpected sources of variation in the data.

```{r}
plotMDS(log2_cpms)
```

### SA plot

As always, residuals are of interest. Here we will plot the residual standard deviation of each gene on a square root scale, versus the average log expression level. We also see the trend line that was fitted during the Empirical Bayes step.

When we used `cpm()` earlier to log transform the data, we gave a "prior count" to damp down variation at low expression levels. We can see there is still some mean-variance trend, but limma was able to model it.

```{r}
plotSA(efit)
```

### Melting the data and using ggplot

For a complex experiment like this, ggplot2 can also be used to look at genes in detail in creative ways. Lets look at the top genes we discovered earlier. To use ggplot, we need to "melt" the data into a "long" data frame. I use `reshape2::melt()` here because the data is in a matrix. If it were in a data frame, I would have used `tidyr::pivot_longer()`.

```{r fig.width=8}
genes_wanted <- rownames(all_results) |> head(20)

melted_df <- 
    melt(log2_cpms, varnames=c("gene","sample"), value.name="log2_cpm") |> 
    filter(gene %in% genes_wanted) |>
    mutate(gene = factor(gene, genes_wanted)) |>
    left_join(teeth, by="sample")

ggplot(melted_df) +
    aes(x=day, y=log2_cpm, color=tooth) +
    facet_wrap(~gene, scale="free_y", ncol=5) +
    geom_line() +
    geom_point()
```

### Heatmap

A heatmap of variable genes can be a great way to reveal features of a dataset. Here I show the top 200 genes selected by range of log2 expression. Expression is shown as z-scores (i.e. scaled to have zero mean, and unit standard deviation). Genes are ordered so as to cluster similar patterns of expression. Heatmaps are a complex topic! There are a lot of alternatives to what I've done here in terms of gene selection, scaling, and clustering.

```{r message=FALSE}
gene_ranges <- apply(log2_cpms,1,max) - apply(log2_cpms,1,min) 
genes_wanted <- gene_ranges |> sort(decreasing=TRUE) |> head(100) |> names()

x <- log2_cpms[genes_wanted,]
z <- t(scale(t(x)))
ordering <- seriate(dist(z), method="OLO")

Heatmap(
    z, 
    name="z score",
    show_row_names=FALSE,
    cluster_rows=as.dendrogram(ordering[[1]]),
    cluster_columns=FALSE)
```

When we looked for genes changing over time, that's what we found. When we let the data speak for itself we may see many strange things! This may point us at important things to include in our model or to normalize out of the data.


## Connections to earlier ideas

### F test

`limma` can also perform tests against a null hypothesis in which several coefficients are dropped from the model, allowing tests like those we have performed with `anova` earlier. Suppose we want to find *any* deviation from a constant expression level. We can check for this by dropping every coefficient except for the intercept. The `eBayes` step is still needed.

```{r}
efit2 <- eBayes(fit, trend=TRUE)
F_test_results <- topTable(efit2, coef=c(2,3,4))
F_test_results
```


### Challenge - visualize different sets of genes {.challenge}

Try visualizing different sets of genes, either using the ggplot2 facet method or the Heatmap method we saw earlier.

Some possible sets of genes are:

* The top genes from the F test we just did, or another test of your own devising.
* The largest log fold changes in the `all_results` table, ignoring p-values.
* A random sample of genes, using `sample()`.


### Relation to lm( ) and glht( )

Let's look at a specific gene.

```{r}
rnf144b <- log2_cpms["Rnf144b",]
rnf144b_fit <- lm(rnf144b ~ tooth * time, data=teeth)

# This is needed to make look() work with the rescaled time
more_data$time <- (more_data$day - 14.5) / (18-14.5)

look(rnf144b, rnf144b_fit)
```

We can use the same linear hypothesis with `glht`. The estimate is the same as reported by `topTable`, but limma gained some power by shrinking the variance toward the trend line, so limma's p-value is smaller.

```{r}
summary( glht(rnf144b_fit, K) )
```


### Confidence intervals

Confidence Intervals may also be of interest. However note that these are not adjusted for multiple testing (see Appendix).

```{r}
topTable(efit, confint=0.95)
```


## Differential expression methods in general usage

### limma-voom

Above, we did a fairly straightforward log transformation of the count data, followed by linear modelling. Personally I think this is adequate for most cases, but some further refinements have been developed.

A step up from this, and probably the most popular method, is "voom". This again works on log transformed data, but uses a precision weight for each individual count, derived from an initial fit of the data, to account for the larger amount of noise associated with lower expression levels on a log scale. With the `eBayes(trend=TRUE)` method we used above, we accounted for this only at the gene level. With "voom", the adjustment is applied at the inidividual count level, based on the predicted expression level from the initial fit and also the library size for each sample, so it is more fine-grained.

```{r}
voomed <- voom(dgelist, design=X, plot=TRUE)
voomed_fit <- lmFit(voomed, design=X)
voomed_cfit <- contrasts.fit(voomed_fit, t(K))
voomed_efit <- eBayes(voomed_cfit)
topTable(voomed_efit)
```

The results with this data are quite similar. Voom may have an advantage where samples have different library sizes. It also has an extension to account for samples of varying quality by estimating "sample weights".

### edgeR

A further step is to use a negative-binomial GLM. Popular packages are DESeq2 and edgeR. Here we'll demonstrate edgeR. edgeR does a number of sophisticated things beyond using a GLM, and the authors seem to always working on it to make it better. Here we use the edgeR "quasi-likelihood" method, their latest iteration.

```{r}
dgelist_with_disp <- estimateDisp(dgelist, X)
edger_fit <- glmQLFit(dgelist_with_disp, X)
edger_results <- glmQLFTest(edger_fit, contrast=t(K))
topTags(edger_results)
```

This is a more principled approach than the log-counts based methods. I would note however that GLMs attempt to fit the mean expected expression levels on a linear scale, and can be overly influenced by large individual counts. We sometimes see "significant" results based on large counts in just one or two samples.

All of these methods produce similar numbers of significant genes with this data.

```{r}
decideTests(efit) |> summary()
decideTests(voomed_efit) |> summary()
decideTests(edger_results) |> summary()
```


## Further exercises {.challenge}

1. Construct and use linear combinations to find genes that:

    A. Differ in slope between lower and upper molars.

    B. Differ in expression at time 0.5 between the lower and upper molars. (Hint: this can be viewed as the difference in predictions between two individual samples.)

    C. Construct a pair of linear combinations that when used together in an F test find genes with non-zero slope in either or both the lower and upper molars.

2. Construct a model matrix in which the change over time is quadratic, and some linear combinations to interrogate it.


# Appendix: more on limma

## Empirical Bayes variance squeezing

In limma, Empirical Bayes squeezing of the residual variance acts as though we have some number of extra "prior" observations of the variance. These are also counted as extra degrees of freedom in F tests. The "prior" observations act to squeeze the estimated residual variance toward a trend line that is a function of the average expression level.

```{r R.options=list(max.print=21)}
efit <- eBayes(cfit, trend=TRUE)

efit$df.prior
efit$df.residual
efit$df.total
plotSA(efit)
points(efit$Amean, efit$s2.post^0.25, col="red", cex=0.2)
```

The total effective degrees of freedom is the "prior" degrees of freedom plus the normal residual degrees of freedom. As can be seen in the plot, compared to the residual variance (black dots), this produces a posterior residual variance (`efit$s2.post`, red dots) that is squeezed toward the trend line.

It's worthwhile checking `df.prior` when using limma, as a low value may indicate a problem with a data-set.


## Top confident effect sizes

A little self-promotion: With limma we are able to find genes where our effect of interest is significantly different from zero. However we may make many thousands of discoveries, too many to easily follow up, and some of the effects discovered may be tiny. I have developed a method to rank genes by confident effect size, with multiple testing correction, implemented in the package topconfects. This method builds on the `treat` method provided by limma.

This is performed with the `limma_confects` function. `limma_confects` incorporates the Empirical Bayes variance squeezing step, so remember to specify `trend=TRUE` if using trend based variance squeezing.

```{r}
library(topconfects)

result <- limma_confects(cfit, "average_slope", trend=TRUE, fdr=0.05)
result
```

Results are ranked by "confect", confident effect size. If you select genes with `abs(confect) >=` some value, those genes will have a magnitude of effect greater than that threshold, with controlled FDR. (Exactly the same set of genes is found using `treat` with this threshold.)

Compared to the "most significant" gene, the top gene by topconfects has a larger slope (but lower overall average expression).

```{r}
hbby <- log2_cpms["Hbb-y",]
hbby_fit <- lm(hbby ~ tooth * day, data=teeth)
look(hbby, hbby_fit)

# Highlight genes with logFC confidently larger than 1.
ggplot(result$table, aes(x=AveExpr, y=effect)) +
    geom_point(size=0.1) +
    geom_point(
        data=filter(result$table, abs(confect) >= 1), 
        size=0.5, color="red")
```

See also "False Coverage-statement Rate" for a generally applicable approach to multiple-testing corrected confidence intervals.

---
```{r}
sessionInfo()
```
